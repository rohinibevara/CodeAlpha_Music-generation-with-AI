# music_generation_lstm.py

import glob
import pickle
import numpy as np
from music21 import converter, instrument, note, chord, stream
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation
from tensorflow.keras.utils import to_categorical

# ----------------------
# Step 1: Load MIDI files
# ----------------------
notes = []

for file in glob.glob("data/midi/*.mid"):
    midi = converter.parse(file)
    parts = instrument.partitionByInstrument(midi)
    if parts:
        notes_to_parse = parts.parts[0].recurse()
    else:
        notes_to_parse = midi.flat.notes

    for element in notes_to_parse:
        if isinstance(element, note.Note):
            notes.append(str(element.pitch))
        elif isinstance(element, chord.Chord):
            notes.append('.'.join(str(n) for n in element.normalOrder))

# Save notes for future use
with open('notes.pkl', 'wb') as f:
    pickle.dump(notes, f)

# ----------------------
# Step 2: Prepare sequences
# ----------------------
sequence_length = 100
pitchnames = sorted(set(notes))
note_to_int = {note: number for number, note in enumerate(pitchnames)}

network_input = []
network_output = []

for i in range(len(notes) - sequence_length):
    seq_in = notes[i:i + sequence_length]
    seq_out = notes[i + sequence_length]
    network_input.append([note_to_int[n] for n in seq_in])
    network_output.append(note_to_int[seq_out])

n_patterns = len(network_input)
network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))
network_input = network_input / float(len(pitchnames))
network_output = to_categorical(network_output)

# ----------------------
# Step 3: Build LSTM model
# ----------------------
model = Sequential()
model.add(LSTM(512, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(len(pitchnames)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam')
model.summary()

# ----------------------
# Step 4: Train model
# ----------------------
model.fit(network_input, network_output, epochs=100, batch_size=64)
model.save('music_lstm_model.h5')

# ----------------------
# Step 5: Generate music
# ----------------------
start = np.random.randint(0, len(network_input)-1)
pattern = network_input[start].tolist()
prediction_output = []

for note_index in range(500):
    prediction_input = np.reshape(pattern, (1, len(pattern), 1))
    prediction_input = prediction_input / float(len(pitchnames))
    
    prediction = model.predict(prediction_input, verbose=0)
    index = np.argmax(prediction)
    result = pitchnames[index]
    prediction_output.append(result)
    
    pattern.append([index])
    pattern = pattern[1:]

offset = 0
output_notes = []
for pattern in prediction_output:
    if '.' in pattern or pattern.isdigit():
        notes_in_chord = pattern.split('.')
        notes_in_chord = [note.Note(int(n)) for n in notes_in_chord]
        new_chord = chord.Chord(notes_in_chord)
        new_chord.offset = offset
        output_notes.append(new_chord)
    else:
        new_note = note.Note(pattern)
        new_note.offset = offset
        output_notes.append(new_note)
    offset += 0.5

midi_stream = stream.Stream(output_notes)
midi_stream.write('midi', fp='generated_music.mid')

print("Music generation complete! Check 'generated_music.mid'.")
